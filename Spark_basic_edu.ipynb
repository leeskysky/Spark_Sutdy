{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f540b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22e8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d27dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('MCB').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "770164f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"TEST.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6d30a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version:  3.2.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession 객체 생성\n",
    "spark = SparkSession.builder.appName(\"SparkVersionCheck\").getOrCreate()\n",
    "\n",
    "# 스파크 버전 확인\n",
    "spark_version = spark.version\n",
    "print(\"Spark Version: \", spark_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2c21c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 이름: string (nullable = true)\n",
      " |-- 나이: integer (nullable = true)\n",
      " |-- 성별: string (nullable = true)\n",
      "\n",
      "+-------+----+----+\n",
      "|   이름|나이|성별|\n",
      "+-------+----+----+\n",
      "|  Alice|  25|여성|\n",
      "|    Bob|  30|남성|\n",
      "|Charlie|  22|남성|\n",
      "|  Diana|  19|여성|\n",
      "|    Eve|  35|여성|\n",
      "+-------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder.appName(\"SparkExample\").getOrCreate()\n",
    "\n",
    "# # CSV 파일로부터 데이터 불러오기\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# # 데이터 프레임 스키마 확인\n",
    "df.printSchema()\n",
    "\n",
    "# # 데이터 프레임의 처음 5개 행 확인\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afe83655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|성별|count(나이)|\n",
      "+----+-----------+\n",
      "|남성|          3|\n",
      "|여성|          3|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.printSchema()\n",
    "\n",
    "# # # 데이터 프레임의 처음 5개 행 확인\n",
    "# df.show(5)\n",
    "\n",
    "\n",
    "# # # # 데이터 프레임의 특정 열 선택\n",
    "# # name_df = df.select(\"이름\")\n",
    "# # name_df.show(5)\n",
    "\n",
    "# # # 나이가 25 이상인 데이터 필터링\n",
    "# age_df = df.filter(df[\"나이\"] >= 25)\n",
    "# age_df.show()\n",
    "\n",
    "# # # 성별별로 그룹화하여 나이의 평균 계산\n",
    "from pyspark.sql.functions import count            # avg, sum, min, max, count\n",
    "\n",
    "age_avg_by_gender = df.groupBy(\"성별\").agg(count(\"나이\"))\n",
    "age_avg_by_gender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad9e9a12",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m col\n\u001b[0;32m      6\u001b[0m \u001b[39m# 스파크 세션 생성\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mSparkExample\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[0;32m      9\u001b[0m \u001b[39m# 예제 데이터 스키마 정의\u001b[39;00m\n\u001b[0;32m     10\u001b[0m schema \u001b[39m=\u001b[39m StructType([\n\u001b[0;32m     11\u001b[0m     StructField(\u001b[39m\"\u001b[39m\u001b[39m이름\u001b[39m\u001b[39m\"\u001b[39m, StringType(), \u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m     12\u001b[0m     StructField(\u001b[39m\"\u001b[39m\u001b[39m나이\u001b[39m\u001b[39m\"\u001b[39m, IntegerType(), \u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     StructField(\u001b[39m\"\u001b[39m\u001b[39m월급\u001b[39m\u001b[39m\"\u001b[39m, IntegerType(), \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m ])\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda\\lib\\site-packages\\pyspark\\sql\\session.py:272\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    269\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    270\u001b[0m     \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    271\u001b[0m     \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m--> 272\u001b[0m     session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_options)\n\u001b[0;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[39mgetattr\u001b[39m(\n\u001b[0;32m    275\u001b[0m         \u001b[39mgetattr\u001b[39m(session\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     )\u001b[39m.\u001b[39mapplyModifiableSettings(session\u001b[39m.\u001b[39m_jsparkSession, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda\\lib\\site-packages\\pyspark\\sql\\session.py:307\u001b[0m, in \u001b[0;36mSparkSession.__init__\u001b[1;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mapplyModifiableSettings(\n\u001b[0;32m    304\u001b[0m             jsparkSession, options\n\u001b[0;32m    305\u001b[0m         )\n\u001b[0;32m    306\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m         jsparkSession \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mSparkSession(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), options)\n\u001b[0;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mapplyModifiableSettings(\n\u001b[0;32m    310\u001b[0m         jsparkSession, options\n\u001b[0;32m    311\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda\\lib\\site-packages\\py4j\\java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1579\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1580\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1581\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1584\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1585\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1586\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[0;32m   1588\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1589\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda\\lib\\site-packages\\py4j\\protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[0;32m    333\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    335\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    336\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# 스파크 세션 생성\n",
    "spark = SparkSession.builder.appName(\"SparkExample\").getOrCreate()\n",
    "\n",
    "# 예제 데이터 스키마 정의\n",
    "schema = StructType([\n",
    "    StructField(\"이름\", StringType(), True),\n",
    "    StructField(\"나이\", IntegerType(), True),\n",
    "    StructField(\"성별\", StringType(), True),\n",
    "    StructField(\"직업\", StringType(), True),\n",
    "    StructField(\"월급\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [('A', 28, \"남\", \"사업가\",300),\n",
    "        ('B', 25, \"여\", \"주부\",200),\n",
    "        ('C', 32, \"여\", \"커리어우먼\",600),\n",
    "        ('D', 23, \"남\", \"과학자\",150),\n",
    "        ('E', 35, \"남\", \"분석가\",180),\n",
    "        ('F', 21, \"여\", \"마음이\",140)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema= schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed4fe212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----------+----+\n",
      "|이름|나이|성별|      직업|월급|\n",
      "+----+----+----+----------+----+\n",
      "|   A|  28|  남|    사업가| 300|\n",
      "|   B|  25|  여|      주부| 200|\n",
      "|   C|  32|  여|커리어우먼| 600|\n",
      "|   E|  35|  남|    분석가| 180|\n",
      "+----+----+----+----------+----+\n",
      "\n",
      "+----+---------+\n",
      "|성별|max(월급)|\n",
      "+----+---------+\n",
      "|  남|      300|\n",
      "|  여|      600|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter 25이상\n",
    "age_over_20_df = df.filter(col(\"나이\") >= 25)\n",
    "age_over_20_df.show()\n",
    "\n",
    "# 성별별 월급 \n",
    "from pyspark.sql.functions import avg, max\n",
    "age_avg_by_gender =  df.groupBy(\"성별\").agg(max('월급'))\n",
    "age_avg_by_gender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d407af7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|성별|인원수|\n",
      "+----+------+\n",
      "|  남|     3|\n",
      "|  여|     3|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "sx_count = df.groupBy(\"성별\").agg(count(\"*\").alias(\"인원수\"))\n",
    "sx_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "005f1867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----------+----+----+\n",
      "|이름|나이|성별|      직업|월급|지역|\n",
      "+----+----+----+----------+----+----+\n",
      "|   A|  28|  남|    사업가| 300|사당|\n",
      "|   B|  25|  여|      주부| 200|홍대|\n",
      "|   C|  32|  여|커리어우먼| 600|수원|\n",
      "|   D|  23|  남|    과학자| 150|약수|\n",
      "|   E|  35|  남|    분석가| 180|약수|\n",
      "|   F|  21|  여|    마음이| 140|안양|\n",
      "+----+----+----+----------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## withcolumn을 활용하여 열 추가\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import concat_ws, lit\n",
    "\n",
    "# 데이터 생성\n",
    "data = [('A', 28, \"남\", \"사업가\", 300),\n",
    "        ('B', 25, \"여\", \"주부\", 200),\n",
    "        ('C', 32, \"여\", \"커리어우먼\", 600),\n",
    "        ('D', 23, \"남\", \"과학자\", 150),\n",
    "        ('E', 35, \"남\", \"분석가\", 180),\n",
    "        ('F', 21, \"여\", \"마음이\", 140)]\n",
    "\n",
    "# DataFrame 생성\n",
    "df = spark.createDataFrame(data, ['이름', '나이', '성별', '직업', '월급'])\n",
    "\n",
    "df = df.withColumn(\"지역\", lit(None))\n",
    "\n",
    "location = [\"강남\",\"홍대\",\"수원\",\"안양\",\"사당\",\"약수\"]\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(len(location)):\n",
    "#     df = df.withColumn(\"지역\", when((df[\"나이\"] % len(location)) == i, lit(location[i])).otherwise(df[\"지역\"]))\n",
    "\n",
    "\n",
    "# df = df.withColumn(\"지역\", when((df[\"나이\"] % len(location)) == 0, location[0] )\n",
    "#                   .when((df[\"나이\"] % len(location)) == 1, location[1])\n",
    "#                   .when((df[\"나이\"] % len(location)) == 2, location[2])\n",
    "#                   .when((df[\"나이\"] % len(location)) == 3, location[3])\n",
    "#                   .when((df[\"나이\"] % len(location)) == 4, location[4])\n",
    "#                   .when((df[\"나이\"] % len(location)) == 5, location[5])\n",
    "            \n",
    "#                   .otherwise(None))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27580006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|이름|나이|\n",
      "+----+----+\n",
      "|   A|  28|\n",
      "|   B|  25|\n",
      "|   C|  32|\n",
      "|   E|  35|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df1 = df.select(\"이름\",\"나이\")\n",
    "df2= df1.filter(col(\"나이\") >= 25)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57bdea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----------+----+----+----+\n",
      "|이름|나이|성별|      직업|월급|지역|순위|\n",
      "+----+----+----+----------+----+----+----+\n",
      "|   C|  32|  여|커리어우먼| 600|수원|   1|\n",
      "|   A|  28|  남|    사업가| 300|사당|   2|\n",
      "|   B|  25|  여|      주부| 200|홍대|   3|\n",
      "|   E|  35|  남|    분석가| 180|약수|   4|\n",
      "|   D|  23|  남|    과학자| 150|약수|   5|\n",
      "|   F|  21|  여|    마음이| 140|안양|   6|\n",
      "+----+----+----+----------+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "windowspec = Window.orderBy(col(\"월급\").desc())\n",
    "windowspec\n",
    "df.withColumn(\"순위\", rank().over(windowspec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46a73477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|성별|              평균|\n",
      "+----+------------------+\n",
      "|  남|28.666666666666668|\n",
      "|  여|              26.0|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "aa = df.groupBy(\"성별\").agg(avg(col(\"나이\")).alias(\"평균\"))\n",
    "aa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52a63c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------------+--------+--------+------+\n",
      "|성별|총월급|         평균월급|최소월급|최대월급|인원수|\n",
      "+----+------+-----------------+--------+--------+------+\n",
      "|  남|   630|            210.0|     150|     300|     3|\n",
      "|  여|   940|313.3333333333333|     140|     600|     3|\n",
      "+----+------+-----------------+--------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, avg, min, max, count\n",
    "\n",
    "# '성별' 컬럼을 기준으로 그룹화하고, '월급' 컬럼에 대해 다양한 집계 함수를 적용\n",
    "result = df.groupBy(\"성별\").agg(sum(\"월급\").alias(\"총월급\"),\n",
    "                                avg(\"월급\").alias(\"평균월급\"),\n",
    "                                min(\"월급\").alias(\"최소월급\"),\n",
    "                                max(\"월급\").alias(\"최대월급\"),\n",
    "                                count(\"*\").alias(\"인원수\"))\n",
    "\n",
    "# 결과 출력\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e886ac5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o26.jdbc.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:33)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:294)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m      8\u001b[0m properties \u001b[39m=\u001b[39m {\n\u001b[0;32m      9\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmyuser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpassword\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmypassword\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdriver\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcom.mysql.jdbc.Driver\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m \u001b[39m# MySQL 데이터 웨어하우스에서 데이터 로드\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread \\\n\u001b[0;32m     16\u001b[0m     \u001b[39m.\u001b[39;49mjdbc(url, \u001b[39m\"\u001b[39;49m\u001b[39mmytable\u001b[39;49m\u001b[39m\"\u001b[39;49m, properties\u001b[39m=\u001b[39;49mproperties)\n\u001b[0;32m     18\u001b[0m \u001b[39m# 데이터 프레임에 대한 분석 또는 처리 수행\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# ...\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[39m# 결과 출력\u001b[39;00m\n\u001b[0;32m     22\u001b[0m df\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\Users\\USER\\dev\\env\\spark-3.2.3-bin-hadoop3.2\\python\\pyspark\\sql\\readwriter.py:529\u001b[0m, in \u001b[0;36mDataFrameReader.jdbc\u001b[1;34m(self, url, table, column, lowerBound, upperBound, numPartitions, predicates, properties)\u001b[0m\n\u001b[0;32m    527\u001b[0m     jpredicates \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mtoJArray(gateway, gateway\u001b[39m.\u001b[39mjvm\u001b[39m.\u001b[39mjava\u001b[39m.\u001b[39mlang\u001b[39m.\u001b[39mString, predicates)\n\u001b[0;32m    528\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader\u001b[39m.\u001b[39mjdbc(url, table, jpredicates, jprop))\n\u001b[1;32m--> 529\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mjdbc(url, table, jprop))\n",
      "File \u001b[1;32mC:\\Users\\USER\\dev\\env\\spark-3.2.3-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Users\\USER\\dev\\env\\spark-3.2.3-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Users\\USER\\dev\\env\\spark-3.2.3-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o26.jdbc.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:33)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:294)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder.appName(\"MySQLExample\").getOrCreate()\n",
    "\n",
    "# MySQL 데이터 웨어하우스에 대한 연결 정보 설정\n",
    "url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
    "properties = {\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# MySQL 데이터 웨어하우스에서 데이터 로드\n",
    "df = spark.read \\\n",
    "    .jdbc(url, \"mytable\", properties=properties)\n",
    "\n",
    "# 데이터 프레임에 대한 분석 또는 처리 수행\n",
    "# ...\n",
    "\n",
    "# 결과 출력\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea2551af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySQL Connection Example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72e7c954",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o422.jdbc.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:33)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:294)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 17\u001b[0m\n\u001b[0;32m     10\u001b[0m properties \u001b[39m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmyuser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpassword\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmypassword\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdriver\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcom.mysql.jdbc.Driver\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     16\u001b[0m \u001b[39m# MySQL 데이터베이스에서 데이터 읽기\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread \\\n\u001b[0;32m     18\u001b[0m     \u001b[39m.\u001b[39;49mjdbc(url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m     19\u001b[0m           table\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m<table_name>\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     20\u001b[0m           properties\u001b[39m=\u001b[39;49mproperties)\n\u001b[0;32m     22\u001b[0m \u001b[39m# 읽은 데이터 확인\u001b[39;00m\n\u001b[0;32m     23\u001b[0m df\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\Users\\USER\\dev\\env\\spark-3.2.3-bin-hadoop3.2\\python\\pyspark\\sql\\readwriter.py:529\u001b[0m, in \u001b[0;36mDataFrameReader.jdbc\u001b[1;34m(self, url, table, column, lowerBound, upperBound, numPartitions, predicates, properties)\u001b[0m\n\u001b[0;32m    527\u001b[0m     jpredicates \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mtoJArray(gateway, gateway\u001b[39m.\u001b[39mjvm\u001b[39m.\u001b[39mjava\u001b[39m.\u001b[39mlang\u001b[39m.\u001b[39mString, predicates)\n\u001b[0;32m    528\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader\u001b[39m.\u001b[39mjdbc(url, table, jpredicates, jprop))\n\u001b[1;32m--> 529\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mjdbc(url, table, jprop))\n",
      "File \u001b[1;32mC:\\Users\\USER\\dev\\env\\spark-3.2.3-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Users\\USER\\dev\\env\\spark-3.2.3-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Users\\USER\\dev\\env\\spark-3.2.3-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o422.jdbc.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:33)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\r\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:294)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark 세션 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySQL Connection Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# MySQL 연결 설정\n",
    "url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
    "properties = {\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# # MySQL 데이터베이스에서 데이터 읽기\n",
    "# df = spark.read \\\n",
    "#     .jdbc(url=url,\n",
    "#           table=\"<table_name>\",\n",
    "#           properties=properties)\n",
    "\n",
    "# # 읽은 데이터 확인\n",
    "# df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e79db5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Spark 세션 생성 시 --jars 옵션을 사용하여 MySQL JDBC 드라이버 지정 예시\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder \\\n\u001b[0;32m      3\u001b[0m     \u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mMySQL Connection Example\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m      4\u001b[0m     \u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.jars\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m/path/to/mysql-connector-java.jar\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m      5\u001b[0m     \u001b[39m.\u001b[39mgetOrCreate()\n\u001b[0;32m      6\u001b[0m spark\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "# Spark 세션 생성 시 --jars 옵션을 사용하여 MySQL JDBC 드라이버 지정 예시\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySQL Connection Example\") \\\n",
    "    .config(\"spark.jars\", \"/path/to/mysql-connector-java.jar\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8c5994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\dev\\env\\spark-3.2.3-bin-hadoop3.2\\python\\pyspark\\sql\\avro\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.avro\n",
    "# import pyspark.sql.jdbc\n",
    "\n",
    "print(pyspark.sql.avro.__file__)    # avro 패키지의 위치 확인\n",
    "# print(pyspark.sql.jdbc.__file__)    # jdbc 패키지의 위치 확인\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30dbb130",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2544524027.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[47], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    public class JDBCDriverLocator {\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b5539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sky",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
