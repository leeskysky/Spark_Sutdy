{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark , mysql DB연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+\n",
      "| id|first_name|last_name|               email|\n",
      "+---+----------+---------+--------------------+\n",
      "|  1|      John|      Doe| johndoe@example.com|\n",
      "|  2|      Jane|    Smith|janesmith@example...|\n",
      "|  3|      Mark|  Johnson|markjohnson@examp...|\n",
      "|  4|        Jo|       Do|    jodo@example.com|\n",
      "|  5|        Ja|      Smi|   jasmi@example.com|\n",
      "|  6|        Ma|      Joh|   majoh@example.com|\n",
      "+---+----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark --jars C:\\env\\mysql-connector-java-8.0.32.jar\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read.format('jdbc').options(         # JDBC 드라이버로 데이터베이스에서 데이터를 읽어옵니다.\n",
    "        driver=\"com.mysql.jdbc.Driver\",         # JDBC 드라이버 클래스 이름을 지정합니다.\n",
    "        url=\"jdbc:mysql://localhost:3306/sky\",  # 데이터베이스의 URL을 지정합니다.\n",
    "        dbtable=\"customers\",                    # 데이터베이스에서 읽어올 테이블의 이름을 지정합니다.\n",
    "        user=\"root\",                            # 데이터베이스에 접속할 때 사용할 사용자 이름을 지정합니다.\n",
    "        password=\"Leehaneul12#\"                 # 데이터베이스에 접속할 때 사용할 비밀번호를 지정합니다.\n",
    "    ) \\\n",
    "    .load()  # 설정한 옵션들을 기반으로 데이터를 로드합니다.\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 새로운 데이터프레임을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+\n",
      "| id|first_name|last_name|               email|\n",
      "+---+----------+---------+--------------------+\n",
      "|  1|      John|      Doe| johndoe@example.com|\n",
      "|  2|      Jane|    Smith|janesmith@example...|\n",
      "|  3|      Mark|  Johnson|markjohnson@examp...|\n",
      "|  4|        Jo|       Do|    jodo@example.com|\n",
      "|  5|        Ja|      Smi|   jasmi@example.com|\n",
      "|  6|        Ma|      Joh|   majoh@example.com|\n",
      "|  7|     Alice|      Kim|   alice@example.com|\n",
      "|  8|       Bob|      jon|     bob@example.com|\n",
      "+---+----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "new_customers_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(),True)\n",
    "])\n",
    "new_customers_data = [(7, \"Alice\", \"Kim\",\"alice@example.com\"), (8, \"Bob\", \"jon\",\"bob@example.com\")]\n",
    "new_customers = spark.createDataFrame(new_customers_data, schema=new_customers_schema)\n",
    "\n",
    "# 불러온 데이터프레임과 새로운 데이터프레임을 합칩니다.\n",
    "merged_df = df.union(new_customers)\n",
    "\n",
    "merged_df.show()\n",
    "\n",
    "# 새로운 테이블을 생성합니다.\n",
    "# merged_df 데이터프레임을 사용하여 merged_customers 임시 테이블을 생성합니다.\n",
    "merged_df.createOrReplaceTempView(\"merged_customers\")\n",
    "merged_df.write.format('jdbc').options(\n",
    "        url='jdbc:mysql://localhost:3306/sky',\n",
    "        driver='com.mysql.jdbc.Driver',\n",
    "        dbtable='ab_customers',\n",
    "        user='root',\n",
    "        password='Leehaneul12#'\n",
    "    ).mode('overwrite').save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+\n",
      "| id|first_name|last_name|               email|\n",
      "+---+----------+---------+--------------------+\n",
      "|  1|      John|      Doe| johndoe@example.com|\n",
      "|  2|      Jane|    Smith|janesmith@example...|\n",
      "|  3|      Mark|  Johnson|markjohnson@examp...|\n",
      "|  4|        Jo|       Do|    jodo@example.com|\n",
      "|  5|        Ja|      Smi|   jasmi@example.com|\n",
      "|  6|        Ma|      Joh|   majoh@example.com|\n",
      "|  7|     Alice|      Kim|   alice@example.com|\n",
      "|  8|       Bob|      jon|     bob@example.com|\n",
      "|  9|       sky|      lee|     sky@example.com|\n",
      "|  9|       sky|      lee|     sky@example.com|\n",
      "|  9|       sky|      lee|     sky@example.com|\n",
      "+---+----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 새로운 데이터를 생성합니다. 행 추가! \n",
    "new_data = [(9, \"sky\", \"lee\", \"sky@example.com\")]\n",
    "\n",
    "# 새로운 데이터를 포함하는 데이터프레임을 생성합니다.\n",
    "new_df = spark.createDataFrame(new_data, [\"id\", \"first_name\", \"last_name\", \"email\"])\n",
    "\n",
    "# 데이터베이스에 새로운 데이터를 추가합니다.\n",
    "new_df.write.format('jdbc').options(\n",
    "        driver='com.mysql.jdbc.Driver',\n",
    "        url='jdbc:mysql://localhost:3306/sky',\n",
    "        dbtable='merged_customers',\n",
    "        user='root',\n",
    "        password='Leehaneul12#'\n",
    "    ).mode('append').save()\n",
    "\n",
    "#확인해보기\n",
    "df = spark.read.format('jdbc').options(         \n",
    "        driver=\"com.mysql.jdbc.Driver\",         \n",
    "        url=\"jdbc:mysql://localhost:3306/sky\",  \n",
    "        dbtable=\"merged_customers\",             \n",
    "        user=\"root\",                            \n",
    "        password=\"Leehaneul12#\"                     ) \\\n",
    "    .load()  # 설정한 옵션들을 기반으로 데이터를 로드합니다.\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 열추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+------+\n",
      "| id|first_name|last_name|               email|gender|\n",
      "+---+----------+---------+--------------------+------+\n",
      "|  1|      John|      Doe| johndoe@example.com|   man|\n",
      "|  2|      Jane|    Smith|janesmith@example...| woman|\n",
      "|  3|      Mark|  Johnson|markjohnson@examp...|   man|\n",
      "|  4|        Jo|       Do|    jodo@example.com|   man|\n",
      "|  5|        Ja|      Smi|   jasmi@example.com| woman|\n",
      "|  6|        Ma|      Joh|   majoh@example.com|   man|\n",
      "|  7|     Alice|      Kim|   alice@example.com| woman|\n",
      "|  8|       Bob|      jon|     bob@example.com| woman|\n",
      "|  9|       sky|      lee|     sky@example.com| woman|\n",
      "|  9|       sky|      lee|     sky@example.com| woman|\n",
      "|  9|       sky|      lee|     sky@example.com| woman|\n",
      "+---+----------+---------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, substring, when\n",
    "\n",
    "df_new = df.withColumn(\"gender\", \n",
    "                       when(substring(col(\"last_name\"), 1, 1).between(\"A\", \"J\"), \"man\")\n",
    "                       .otherwise(\"woman\"))\n",
    "\n",
    "df_new.createOrReplaceTempView(\"merged_customers\")\n",
    "df_new.write.format('jdbc').options(\n",
    "        url='jdbc:mysql://localhost:3306/sky',\n",
    "        driver='com.mysql.jdbc.Driver',\n",
    "        dbtable='ab_customers',\n",
    "        user='root',\n",
    "        password='Leehaneul12#'\n",
    "    ).mode('overwrite').save()\n",
    "\n",
    "#확인해보기\n",
    "df = spark.read.format('jdbc').options(         \n",
    "        driver=\"com.mysql.jdbc.Driver\",         \n",
    "        url=\"jdbc:mysql://localhost:3306/sky\",  \n",
    "        dbtable=\"ab_customers\",             \n",
    "        user=\"root\",                            \n",
    "        password=\"Leehaneul12#\"                     ) \\\n",
    "    .load()  # 설정한 옵션들을 기반으로 데이터를 로드합니다.\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark 내장함수와 SQL문. 둘 다 같은 결과를 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.createTempView('df')\n",
    "df_sql = spark.sql(\"\"\"SELECT *,   \n",
    "    CASE \n",
    "        WHEN substring(last_name, 1, 1) BETWEEN 'A' AND 'J' THEN 'man'\n",
    "        ELSE 'woman'\n",
    "    END AS gender\n",
    "FROM df\"\"\")\n",
    "\n",
    "#위는 sql문, 밑은 spark 내장 함수를 활용\n",
    "\n",
    "df_new = df.withColumn(\"gender\", \n",
    "                       when(substring(col(\"last_name\"), 1, 1).between(\"A\", \"J\"), \"man\")\n",
    "                       .otherwise(\"woman\"))\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+\n",
      "| id|name|               email|\n",
      "+---+----+--------------------+\n",
      "|  1|John| johndoe@example.com|\n",
      "|  2|Jane|janesmith@example...|\n",
      "|  3|Mark|markjohnson@examp...|\n",
      "|  4|  Jo|   jasmi@example.com|\n",
      "+---+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 테이블 생성\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "data1 = [(1, \"John\"), (2, \"Jane\"), (3, \"Mark\"), (4, \"Jo\")]\n",
    "schema1 = StructType([StructField(\"id\", IntegerType(), True), StructField(\"name\", StringType(), True)])\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "\n",
    "# 두번째 테이블 생성\n",
    "data2 = [(1, \"johndoe@example.com\"), (2, \"janesmith@example.com\"), (3, \"markjohnson@example.com\"), (4, \"jasmi@example.com\")]\n",
    "schema2 = StructType([StructField(\"id\", IntegerType(), True), StructField(\"email\", StringType(), True)])\n",
    "df2 = spark.createDataFrame(data2, schema2)\n",
    "\n",
    "# join 연습\n",
    "df_join = df1.join(df2, [\"id\"])\n",
    "df_join.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+------+\n",
      "| id|first_name|last_name|               email|gender|\n",
      "+---+----------+---------+--------------------+------+\n",
      "|  1|      John|      Doe| johndoe@example.com|   man|\n",
      "|  2|      Jane|    Smith|janesmith@example...| woman|\n",
      "|  3|      Mark|  Johnson|markjohnson@examp...|   man|\n",
      "|  4|        Jo|       Do|    jodo@example.com|   man|\n",
      "|  5|        Ja|      Smi|   jasmi@example.com| woman|\n",
      "|  6|        Ma|      Joh|   majoh@example.com|   man|\n",
      "|  7|     Alice|      Kim|   alice@example.com| woman|\n",
      "|  8|       Bob|      jon|     bob@example.com| woman|\n",
      "|  9|       sky|      lee|     sky@example.com| woman|\n",
      "|  9|       sky|      lee|     sky@example.com| woman|\n",
      "|  9|       sky|      lee|     sky@example.com| woman|\n",
      "+---+----------+---------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| ID|department|\n",
      "+---+----------+\n",
      "|  0|    Market|\n",
      "|  1|  economic|\n",
      "|  2|  economic|\n",
      "|  3|    Market|\n",
      "|  4|     Clean|\n",
      "|  5|  economic|\n",
      "|  6|    Market|\n",
      "|  7|   Service|\n",
      "|  8|     Clean|\n",
      "|  9|   Service|\n",
      "| 10|     Clean|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "import random\n",
    "# part 컬럼 추가\n",
    "\n",
    "part = [\"Service\", \"Clean\", \"Market\", \"economic\"]\n",
    "\n",
    "result = [Row(ID=i, department=part[random.randint(0, 3)]) for i in range(df_new.count())]\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Random Dataframe\").getOrCreate()\n",
    "\n",
    "df_part = spark.createDataFrame(result)\n",
    "\n",
    "df_part.show()\n",
    "\n",
    "# join 연습\n",
    "df_join_part = df_new.join(df_part, [\"id\"]) \n",
    "df_join_part.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeStamp 찍기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+------+----------+----------+\n",
      "| id|first_name|last_name|               email|gender|department| Timestamp|\n",
      "+---+----------+---------+--------------------+------+----------+----------+\n",
      "|  5|        Ja|      Smi|   jasmi@example.com| woman|  economic|2023-04-20|\n",
      "|  9|       sky|      lee|     sky@example.com| woman|   Service|2023-04-20|\n",
      "|  2|      Jane|    Smith|janesmith@example...| woman|  economic|2023-04-20|\n",
      "|  7|     Alice|      Kim|   alice@example.com| woman|   Service|2023-04-20|\n",
      "|  3|      Mark|  Johnson|markjohnson@examp...|   man|    Market|2023-04-20|\n",
      "|  1|      John|      Doe| johndoe@example.com|   man|  economic|2023-04-20|\n",
      "|  6|        Ma|      Joh|   majoh@example.com|   man|    Market|2023-04-20|\n",
      "|  4|        Jo|       Do|    jodo@example.com|   man|     Clean|2023-04-20|\n",
      "|  8|       Bob|      jon|     bob@example.com| woman|     Clean|2023-04-20|\n",
      "+---+----------+---------+--------------------+------+----------+----------+\n",
      "\n",
      "+---+----------+---------+--------------------+------+----------+----------+\n",
      "| id|first_name|last_name|               email|gender|department| Timestamp|\n",
      "+---+----------+---------+--------------------+------+----------+----------+\n",
      "|  1|      John|      Doe| johndoe@example.com|   man|  economic|2023-04-20|\n",
      "|  2|      Jane|    Smith|janesmith@example...| woman|  economic|2023-04-20|\n",
      "|  3|      Mark|  Johnson|markjohnson@examp...|   man|    Market|2023-04-20|\n",
      "|  4|        Jo|       Do|    jodo@example.com|   man|     Clean|2023-04-20|\n",
      "|  5|        Ja|      Smi|   jasmi@example.com| woman|  economic|2023-04-20|\n",
      "|  6|        Ma|      Joh|   majoh@example.com|   man|    Market|2023-04-20|\n",
      "|  7|     Alice|      Kim|   alice@example.com| woman|   Service|2023-04-20|\n",
      "|  8|       Bob|      jon|     bob@example.com| woman|     Clean|2023-04-20|\n",
      "|  9|       sky|      lee|     sky@example.com| woman|   Service|2023-04-20|\n",
      "|  9|       sky|      lee|     sky@example.com| woman|   Service|2023-04-20|\n",
      "|  9|       sky|      lee|     sky@example.com| woman|   Service|2023-04-20|\n",
      "+---+----------+---------+--------------------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date,date_format\n",
    "# # df_join_part = df_join_part.drop(\"Timestamp\") #drop 예제\n",
    "\n",
    "\n",
    "df_join_part = df_join_part.withColumn(\"Timestamp\",date_format(current_date(), \"yyyy-MM-dd\"))\n",
    "df_drop_duplicates = df_join_part.dropDuplicates()  #중복된 행 제거\n",
    "df_drop_duplicates.show()\n",
    "\n",
    "df_drop_duplicates.createOrReplaceTempView(\"distinct\")\n",
    "df_drop_duplicates.write.format('jdbc').options(\n",
    "        url='jdbc:mysql://localhost:3306/sky',\n",
    "        driver='com.mysql.jdbc.Driver',\n",
    "        dbtable='part1',\n",
    "        user='root',\n",
    "        password='Leehaneul12#'\n",
    "    ).mode('overwrite').save()\n",
    "df_drop_duplicates.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+------+----------+-------------------+\n",
      "| ID|first_name|last_name|               email|gender|department|          Timestamp|\n",
      "+---+----------+---------+--------------------+------+----------+-------------------+\n",
      "|  1|      John|      Doe| johndoe@example.com|   man|  economic|2023-04-20 00:00:00|\n",
      "|  2|      Jane|    Smith|janesmith@example...| woman|  economic|2023-04-20 00:00:00|\n",
      "|  3|      Mark|  Johnson|markjohnson@examp...|   man|    Market|2023-04-20 00:00:00|\n",
      "|  4|        Jo|       Do|    jodo@example.com|   man|     Clean|2023-04-20 00:00:00|\n",
      "|  5|        Ja|      Smi|   jasmi@example.com| woman|  economic|2023-04-20 00:00:00|\n",
      "|  6|        Ma|      Joh|   majoh@example.com|   man|    Market|2023-04-20 00:00:00|\n",
      "|  7|     Alice|      Kim|   alice@example.com| woman|   Service|2023-04-20 00:00:00|\n",
      "|  8|       Bob|      jon|     bob@example.com| woman|     Clean|2023-04-20 00:00:00|\n",
      "|  9|       sky|      lee|     sky@example.com| woman|   Service|2023-04-20 00:00:00|\n",
      "+---+----------+---------+--------------------+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType,TimestampType,StringType\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "from pyspark.sql.functions import asc\n",
    "\n",
    "aaa  = df_drop_duplicates.select(\n",
    "    df_drop_duplicates[\"ID\"].cast(IntegerType()),\n",
    "    df_drop_duplicates[\"first_name\"],\n",
    "    df_drop_duplicates[\"last_name\"],\n",
    "    df_drop_duplicates[\"email\"],\n",
    "    df_drop_duplicates[\"gender\"],\n",
    "    df_drop_duplicates[\"department\"].cast(StringType()),\n",
    "    df_drop_duplicates[\"Timestamp\"].cast(TimestampType())\n",
    ")\n",
    "\n",
    "ab =aaa.orderBy(asc(\"ID\"))\n",
    "ab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sky",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
